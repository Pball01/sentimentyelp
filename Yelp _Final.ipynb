{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priankaball/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priankaball/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/priankaball/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string\n",
    "import plotly\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from langdetect import detect_langs\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "nltk.download('vader_lexicon')\n",
    "import sklearn.model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rows(filepath, nrows = None):\n",
    "    with open(filepath) as json_file:\n",
    "        count = 0\n",
    "        objs = []\n",
    "        line = json_file.readline()\n",
    "        while (nrows is None or count < nrows) and line:\n",
    "            count += 1\n",
    "            obj = json.loads(line)\n",
    "            objs.append(obj)\n",
    "            line = json_file.readline()\n",
    "        return pd.DataFrame(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "business = load_rows('yelp_academic_dataset_business.json',10000)\n",
    "review = load_rows('yelp_academic_dataset_review.json',10000)\n",
    "business = business[business['is_open']==1] #selecting only open restaurants\n",
    "business = business.drop(['hours','is_open','address', 'postal_code', 'attributes', 'review_count',  'city', 'latitude','longitude'], axis=1)\n",
    "#filter for restaurnats\n",
    "business_res = business[business['categories'].str.contains(\n",
    "              'Restaurants',\n",
    "              case=False, na=False)]\n",
    "business_res = business_res.rename(columns={'stars': 'business_stars'})\n",
    "#removing columns that might not be relevant\n",
    "review = review.drop(['useful','user_id','funny', 'cool', 'date'], axis=1)\n",
    "review = review.rename(columns={'stars': 'review_stars'})\n",
    "final = pd.merge(business_res, review, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5b6b34cf039d>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['language'] = languages\n"
     ]
    }
   ],
   "source": [
    "#selecting stars and text of reviews\n",
    "df1 = final[['review_stars', 'text']]\n",
    "\n",
    "#We know from past analysis that there is foreign language in this dataset. \n",
    "language = [detect_langs(i) for i in df1.text]\n",
    "languages = [str(i[0]).split(':')[0] for i in language]\n",
    "df1['language'] = languages\n",
    "df1 = df1.loc[df1[\"language\"].isin([\"en\"])]\n",
    "\n",
    "\n",
    "# Create the dictionary \n",
    "label_dictionary ={1 : 1, 2 : 1, 3 : 1,\n",
    "                   4 : 0, 5 : 0} \n",
    "\n",
    "# Add a new column named Label \n",
    "df1['label'] = df1['review_stars'].map(label_dictionary)\n",
    "\n",
    "df2 = df1[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"u\": \"you\", #added this from the analysis\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\") + list(ENGLISH_STOP_WORDS)+\n",
    "                   ['will', 'still'])\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "#using lemmetization\n",
    "#def lemmatized_words(text):\n",
    "#    lemm = nltk.stem.WordNetLemmatizer()\n",
    "#    df2['lemmatized_text'] = list(map(lambda word:\n",
    "#                                     list(map(lemm.lemmatize, word)),\n",
    "#                                     df2.Text_Cleaned))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f2608ff35743>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Text_Cleaned'] = list(map(clean_text, df2.text))\n"
     ]
    }
   ],
   "source": [
    "df2['Text_Cleaned'] = list(map(clean_text, df2.text))\n",
    "#lemmatized_words(df2.Text_Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[502  38]\n",
      " [ 85 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       540\n",
      "           1       0.81      0.66      0.72       247\n",
      "\n",
      "    accuracy                           0.84       787\n",
      "   macro avg       0.83      0.79      0.81       787\n",
      "weighted avg       0.84      0.84      0.84       787\n",
      "\n",
      "0.843710292249047\n"
     ]
    }
   ],
   "source": [
    "#Bag of words transformation\n",
    "training_data, test_data = sklearn.model_selection.train_test_split(df2, train_size = 0.7, random_state=42)\n",
    "#bag of words transforming training and test data\n",
    "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False)\n",
    "X_tr_bow = bow_transform.fit_transform(training_data['Text_Cleaned']) #training data\n",
    "X_te_bow = bow_transform.transform(test_data['Text_Cleaned']) #test data\n",
    "\n",
    "y_tr = training_data['label']\n",
    "y_te = test_data['label']\n",
    "\n",
    "\n",
    "#NaiveBayes with bag of words transformation\n",
    "#This is the best model\n",
    "nb_classifier = MultinomialNB(alpha = 0.9, fit_prior = False)\n",
    "nb_classifier.fit(X_tr_bow, y_tr)\n",
    "y_pred = nb_classifier.predict(X_te_bow)\n",
    "\n",
    "print(confusion_matrix(y_te,y_pred))\n",
    "print(classification_report(y_te,y_pred))\n",
    "print(accuracy_score(y_te, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
